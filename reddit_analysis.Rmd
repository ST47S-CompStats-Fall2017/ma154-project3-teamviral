---
title: "Investigating the Popularity of Scientific Phenomena on Social Media Platforms"
author: "Sal Fu, Brian Lorenz, Jerry Xuan"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, cache=TRUE, 
                      fig.width=7, fig.height=3, fig.align = "center")
```

## Section 1: Motivation

Large numbers of scientific studies are published each day, many of which address pertinent gaps in human knowledge. Some results, however, become much more visible to the general populace than others. Why do some scientific results go viral, while others remain obscure? We wish to investigate what factors might influence the popularity of links about science on a social media platform and the insights that may lend to scientific communication.

We choose Reddit as our platform for analysis because it is the 8th most visited site around the world, with over 500 million users total. This particular platform is convenient because it has a specific forum, the "Science" subreddit, that is dedicated to sharing and discussing scientific results. This subreddit can also reach a broad viewership because all Reddit users are subscribed to it by default (at the time this was written, the "Science" subreddit has around 18 million subscribers). 

On Reddit, users can post content and "Upvote" the posts that they like. Within a subreddit, posts with higher upvotes will end up on the first page of the subreddit (e.g. they are the first posts that users will see when they visit the subreddit). When a post in "Science" gathers enough votes, it will end up on the "Reddit" frontpage, where it will get an even large viewership (i.e., all Reddit users will see the post when they open the website). 

For this work, we will use "Upvotes" as our metric of whether a scientific article is popular: i.e., our response variable. We plan to build a random forest model to understand which variables may contribute the most to whether a scientific article garners a large number of votes.

The structure of this document is as follows: In section 2, we describe how we acquire data from Reddit, and how we use that data to acquire more information about the posts. In section 3, we describe the process of manipulating various columns to make our model more robust. In section 4, we describe the process of model building and interpretation. In Section 5, we summarize our results and interpret them in the context of our broader question.  

## Section 2: Data Acquisition and Modification

Using the Python Reddit API Wrapper, we obtain data about individual posts in the "Science" subreddit for the past two years (from Nov 2015 to Nov 2017). We query the number of upvotes that a post has, as that will be our metric of popularity. We also query for fields that represent explanatory variables, variables which could potentially be used to predict a post's popularity. For instance, these include the title of the post, the time at which the post was created, the amount of "karma" the post's author has, the number of comments on the post, the scientific subfield of the post, and so on.

With the fields that we queried as a basis, we extract additional information from them. For example, as we acquired data from the API, we create an explanatory variable called the "journal $h$ index", which tracks posts that link to "high-impact" scientific articles. For this, we reference the $h$ index rankings from Scimago Country and Rank (scimagojr.com), and define posts with a high $h$ index as those that link to journals which are in the top 10 of the $h$ index rankings. When attempting to download the data, some explanatory variables were not in a readable format for a .csv file, so they required encoding. However, a few characters in the past two years of data broke that encoding. As a result, we acquired the data from November 2015-November 2017 in 9 chunks. The results from the API query are saved in the repository as `reddit_df_final[number].csv`, with `[number]` ranging from 1 to 9 in the order that they finished downloading (not necessarily chronological). 

The code chunk below will run our python data aquisition code if you set eval=TRUE, though it must be modified with the start and end times listed in the code comments and run nine times if you wish to recreate our dataset exactly. However, our entire dataset is also available in this GitHub repository, so there is no need to rerun this chunk. 

```{r, engine='python',echo=FALSE,eval=TRUE}
# Created reddit account "compstatsf17" with password "Sagehen47!" (no quotes on either of those)
#
# registered app as compstats17_final_project
# client_secret: WAe0pEENwYhWu6tErofV7T7QOR4
# client_id: rNGwNS4hRPJDfw
#
# Connected app with the account compstatsf17, so now the app can access all functions of that user
# The "state" of the user is the pseudo-random string (no quotes) "fenwiosj"
# The code created when authorizing the user is (no quotes) "yE0L1_clkxrpp0mVpVthf-KQWlo"
#
# The python api wrapper can be found here:
# https://github.com/praw-dev/praw

import pprint
import praw
import pandas as pd

# from IPython.display import display, HTML

reddit = praw.Reddit(client_id='rNGwNS4hRPJDfw', client_secret="WAe0pEENwYhWu6tErofV7T7QOR4",
                     password='Sagehen47!', user_agent='python:rNGwNS4hRPJDfw:v1.0 (by /u/compstats17)',
                     username='compstatsf17')

#print(reddit.user.me())

#for submission in reddit.front.hot(limit=10):

## Convert to epoch timestamp via https://www.epochconverter.com/
## Retrieving data from the past 2 years:

##Due to errors with the reddit post characters, we had to run this in 9 chunks. The start and end times can be found here, and should be modified along with the csv_path below to recreate our data exactly:
#Number      #End        #Start
#1:       1512097409    1496302522
#2:       1496291678    1493729485
#3:       1480478654    1469185637
#4:       1464695476    1448900355
#5:       1469098822    1467287747
#6:       1491999970    1491999970
#7:       1487099881    1483710465
#8:       1467099725    1464698495
#9:       1483597268    1480509330

end_time = 1512614932
start_time = 1449400000


# Columns for dataframe
id_list = []
url_list = []
title_list = []
title_len_list = []
score_list = []
domain_list = []
subfield_list = []
author_list = []
link_karma_list = []
comment_karma_list = []
author_created_list = []
author_flair_list = []
author_flair_binary_list = []
total_upvotes_list = []
created_utc_list = []
num_comments_list = []
gilded_list = []
image_list = []
journal_h_index_list = []

count = 0
## Searching posts from a given period of time in the SCIENCE subreddit
for submission in reddit.subreddit('science').submissions(
        start=start_time, end=end_time, extra_query=None):

    count = count + 1

    # The images in the preview-images subdictionary are copies of the same image
    # with different sizes. Thus, making preview-images a binary variable makes more sense.
    try:
        preview = vars(submission)['preview']['images'][0]
        image_list.append('yes')
    except:
        # print ('No images for this post')
        image_list.append('no')

    # Populate lists. I had to encode these to make the .to_csv work.
    id_list.append(submission.id.encode('utf-8'))
    url_list.append(submission.url.encode('utf-8'))
    # Gives funny characters in csv file
    title_list.append(submission.title.encode('utf-8'))
    title_len = len((submission.title.encode('utf-8')).replace(' ',''))
    title_len_list.append(title_len)
    score_list.append(submission.score)

    domain = vars(submission)['domain']
    domain_list.append(domain)
    # Binary variable for high impact journals and low impact journal/mere websites
    # Journal impact determined by its h-index. See here for a ranking:
    # http://www.scimagojr.com/journalrank.php?order=h&ord=desc

    # List of substrings that the high impact journals (h-index > 500) would contain
    h_index_500 = ['nature.com', 'sciencemag.org', 'nejm.org', 'cell.com', 'pnas.org',
                   'thelancet.com', 'jamanetwork.com', 'aps.org', 'acs.org', 'circ.ahajournals.org']

    if any(substring in domain for substring in h_index_500):
        journal_h_index_list.append('high')
    else:
        journal_h_index_list.append('low')

    subfield_list.append(vars(submission)['link_flair_text'])

    author = str(vars(submission)['author'])
    author_list.append(author)

    # Variables associated with the author of the post
    user = reddit.redditor(author)
    #print (user)

    # Some authors are no longer on reddit,
    # so they return errors for the author info queries below.
    # We ignore these authors by assigining NA to the following 3 fields.
    try:
        link_karma_list.append(user.link_karma)
    except:
        link_karma_list.append('NA')
    try:
        comment_karma_list.append(user.comment_karma)
    except:
        comment_karma_list.append('NA')
    try:
        author_created_list.append(user.created_utc)
    except:
        author_created_list.append('NA')


    author_flair = vars(submission)['author_flair_css_class']
    if not author_flair == None:
        if len(author_flair) > 0:
            author_flair_binary = 'yes'
        else:
            author_flair_binary = 'no'
    else:
        author_flair_binary = 'no'

    author_flair_binary_list.append(author_flair_binary)
    author_flair_list.append(author_flair)
    total_upvotes_list.append(vars(submission)['ups'])
    created_utc = vars(submission)['created_utc']
    created_utc_list.append(created_utc)
    if count % 100 == 0:
        print ('Currently at epoch time of:')
        print (created_utc)

        print (len(author_list))
        print (len(link_karma_list))
        print (len(comment_karma_list))
        print (len(author_created_list))
        print (len(image_list))

    num_comments_list.append(vars(submission)['num_comments'])
    gilded_list.append(vars(submission)['gilded'])



# Consolidate lists into one pandas data frame
reddit_df = pd.DataFrame(
    {'id': id_list,
     'url': url_list,
     'title': title_list,
     'title_len': title_len_list,
     'domain': domain_list,
     'journal_h_index': journal_h_index_list,
     'subfield': subfield_list,
     'author': author_list,
     'link_karma': link_karma_list,
     'comment_karma': comment_karma_list,
     'author_created_date': author_created_list,
     'author_flair': author_flair_list,
     'author_flair_binary': author_flair_binary_list,
     'upvotes': total_upvotes_list,
     'created_utc': created_utc_list,
     'num_comments': num_comments_list,
     'gilded': gilded_list,
     'image': image_list,
    })

# Check out the dimensions of the dataframe
print (reddit_df.shape)

# Save the dataframe as a csv file

csv_path = './reddit_df.csv'
reddit_df.to_csv(csv_path)
```

After we have generated this dataset, we work to extract additional information out of the post titles. First, we calculate the total length, mean word length, and max word length of each title. Then, we perform sentiment analysis on the titles to obtain a sentiment score for each title. All these became new explanatory variables for our data.

The title sentiment analysis works as follows: we obtain the corpus of words from SentiWordNet, which assigns nearly every word in the English language a positive or negative index with a value between 0 and 1. From each of our post titles, we remove "stopwords" such as "about" and "the" (words which do not have positive or negative connotations). Taking words with an overall positive meaning to have a positive value, and treating negatively-connotating words analogously, we sum up the total indices of the remaining words in the title. We then divide that sum by the number of words in the title to acquire our desired variable, which can be thought of as the title's sentiment "density". 

As the sentiment code takes multiple hours to run on our laptops, we have performed the analysis and saved the results in the repository as `reddit_df_final[number]_sent.csv`, with `[number]` matching the number index of the dataframe. In the interest of reproducibility, we have left the chunk toggleable below. If you set eval=TRUE, the chunk will rerun sentiment analysis on our nine dataframes. 

```{r,echo=FALSE}
#Reading in the packages that we used to wrangle our data

require(lubridate)
require(dplyr)
library(readr)
require(ggplot2)
library(stringr)
library("splitstackshape")
require(caret)
require(rpart)
```

```{r, echo=FALSE,eval=FALSE}
reddit_df1 <- read_csv("reddit_df_final1.csv")
reddit_df2 <- read_csv("reddit_df_final2.csv")
reddit_df3 <- read_csv("reddit_df_final3.csv")
reddit_df4 <- read_csv("reddit_df_final4.csv")
reddit_df5 <- read_csv("reddit_df_final5.csv")
reddit_df6 <- read_csv("reddit_df_final6.csv")
reddit_df7 <- read_csv("reddit_df_final7.csv")
reddit_df8 <- read_csv("reddit_df_final8.csv")
reddit_df9 <- read_csv("reddit_df_final9.csv")

#Sentiment Analysis on title
#Read the SentiWordNet file:
SentiWordNet <- read_delim("SentiWordNet_3.0.0_20130122.txt","\t",skip = 26, escape_double = FALSE, trim_ws = TRUE)

#Save a list of stopwords (credit to xpo6.com)
stopwords <- c("a", "about", "above", "above", "across", "after", "afterwards", "again", "against", "all", "almost", "along", "already", "also","although","always","am","among", "amongst", "amoungst", "amount",  "an", "and", "another", "any","anyhow","anyone","anything","anyway", "anywhere", "are", "around", "as",  "at", "back","be","became", "because","become","becomes", "becoming", "been", "before", "beforehand", "behind", "being", "below", "beside", "besides", "between", "beyond", "both", "bottom","but", "by", "call", "can", "cannot", "cant", "co", "con", "could", "couldnt", "cry", "de", "detail", "do", "done", "down", "due", "during", "each", "eg", "eight", "either", "eleven","else", "elsewhere", "empty", "enough", "etc", "even", "ever", "every", "everyone", "everything", "everywhere", "except", "few", "fifteen", "fify", "fill", "find", "fire", "first", "five", "for", "former", "formerly", "forty", "found", "four", "from", "front", "full", "further", "get", "give", "go", "had", "has", "hasnt", "have", "he", "hence", "her", "here", "hereafter", "hereby", "herein", "hereupon", "hers", "herself", "him", "himself", "his", "how", "however", "hundred", "ie", "if", "in", "inc", "indeed", "interest", "into", "is", "it", "its", "itself", "keep", "last", "latter", "latterly", "least", "less", "ltd", "made", "many", "may", "me", "meanwhile", "might", "mill", "mine", "more", "moreover", "most", "mostly", "move", "much", "must", "my", "myself", "name", "namely", "neither", "never", "nevertheless", "next", "nine", "no", "nobody", "none", "noone", "nor", "not", "nothing", "now", "nowhere", "of", "off", "often", "on", "once", "one", "only", "onto", "or", "other", "others", "otherwise", "our", "ours", "ourselves", "out", "over", "own","part", "per", "perhaps", "please", "put", "rather", "re", "same", "see", "seem", "seemed", "seeming", "seems", "serious", "several", "she", "should", "show", "side", "since", "sincere", "six", "sixty", "so", "some", "somehow", "someone", "something", "sometime", "sometimes", "somewhere", "still", "such", "system", "take", "ten", "than", "that", "the", "their", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "therefore", "therein", "thereupon", "these", "they", "thickv", "thin", "third", "this", "those", "though", "three", "through", "throughout", "thru", "thus", "to", "together", "too", "top", "toward", "towards", "twelve", "twenty", "two", "un", "under", "until", "up", "upon", "us", "very", "via", "was", "we", "well", "were", "what", "whatever", "when", "whence", "whenever", "where", "whereafter", "whereas", "whereby", "wherein", "whereupon", "wherever", "whether", "which", "while", "whither", "who", "whoever", "whole", "whom", "whose", "why", "will", "with", "within", "without", "would", "yet", "you", "your", "yours", "yourself", "yourselves", "the")

#Alter SentiWordNet dataframe to have one word per row instead of multiple
library("splitstackshape")
SentiWordNet <- cSplit(SentiWordNet,"SynsetTerms"," ",direction = "long")
SentiWordNet$SynsetTerms <- as.character(SentiWordNet$SynsetTerms) 

analyzeTitle <- function(reddit_df,indexNum) {

  #Set up vectors for title analysis results
  c_test <- str_split(reddit_df$title,boundary("word"))
  max_vec <- c()
  mean_vec <- c()
  titleSent_vec <- c()
  
  for (i in 1:length(c_test)){
    #Compute maximum and mean word length
    title_vec <- unlist(c_test[i])
    max_word_len <- max(nchar(title_vec))
    mean_word_len <- mean(nchar(title_vec))
    max_vec <- c(max_vec, max_word_len)
    mean_vec <- c(mean_vec, mean_word_len)
  
    #Sentiment Analysis
    titleScore <- c()
    for (word in title_vec){
      if (word %in% stopwords)
      {
        
      }
      else
      {
        word <- str_to_lower(word)
        #Add #1 to get first instance of word
        word <- paste(word, "#1",sep = "")
        #Find the word matrix for the word
        wordMatrix <- SentiWordNet[grep(word, SentiWordNet$SynsetTerms),]
        #Finds the shortest entry in the matrix (gives "thing#1" instead of "anything#1")
        wordrow <- wordMatrix[nchar(wordMatrix$SynsetTerms) == min(nchar(wordMatrix$SynsetTerms)),][1,]
        #Compte the score for this word
        wordScore <- wordrow$PosScore - wordrow$NegScore
        #Append to vector for this title
        titleScore <- c(titleScore, wordScore)
      }
    }
    #Compute and append mean sentiment score for this title
    titleSent_vec <- c(titleSent_vec, mean(titleScore,na.rm=TRUE))
  }
  #c_test <- gsub("<", "", c_test, fixed=TRUE)
  reddit_df$max_title_length <- max_vec
  reddit_df$mean_title_length <- mean_vec
  reddit_df$title_sent <- titleSent_vec
  
  savename <- paste("reddit_df_final",as.character(indexNum),"_sent.csv",sep = "")

  write.csv(reddit_df, file = savename)
}

#Run the above function on all of the data. It saves the sentiment analysis in a separate file to be read in below. 
analyzeTitle(reddit_df1,1)
analyzeTitle(reddit_df2,2)
analyzeTitle(reddit_df3,3)
analyzeTitle(reddit_df4,4)
analyzeTitle(reddit_df5,5)
analyzeTitle(reddit_df6,6)
analyzeTitle(reddit_df7,7)
analyzeTitle(reddit_df8,8)
analyzeTitle(reddit_df9,9)
```

```{r,echo=FALSE}
#Read the reddit data into a dataframe after sentiment analysis
reddit_df_final1 <- read_csv("reddit_df_final1_sent.csv")
reddit_df_final2 <- read_csv("reddit_df_final2_sent.csv")
reddit_df_final3 <- read_csv("reddit_df_final3_sent.csv")
reddit_df_final4 <- read_csv("reddit_df_final4_sent.csv")
reddit_df_final5 <- read_csv("reddit_df_final5_sent.csv")
reddit_df_final6 <- read_csv("reddit_df_final6_sent.csv")
reddit_df_final7 <- read_csv("reddit_df_final7_sent.csv")
reddit_df_final8 <- read_csv("reddit_df_final8_sent.csv")
reddit_df_final9 <- read_csv("reddit_df_final9_sent.csv")

#Bind all of the dataframes together into one final dataframe.   
reddit_df <- rbind(reddit_df_final1,reddit_df_final2,reddit_df_final3,
                     reddit_df_final4,reddit_df_final5,reddit_df_final6,
                     reddit_df_final7,reddit_df_final8,reddit_df_final9)
```

We wanted to get a sense for whether sentiment analysis worked as expected, so we looked at the titles with the most "positive" sentiment and the most "negative" sentiments. The top five positive titles are below:

```{r}
reddit_df %>%
  arrange(desc(title_sent)) %>%
  select(title,title_sent) %>%
  head(n=5L)
```

The most positive two titles are: "Nonnutritive sweeteners and cardiometabolic health" and "Babies know when they don't know something". Some of these are reasonable? But others are: "Happiness doesn't bring good health, study finds," and "Facebook posts inspired by envy, UBC study finds." For some of these, we can see where the sentiment analysis failed. For instance, "Happiness doesn't bring good health" has positive words like "happiness," "good," and "health", which can make the average sentiment of the title positive even if the presence of the single negation "doesn't" alters the overall meaning of the statement. 

Now, we print the five posts with the most negative title sentiments:

```{r}
reddit_df %>%
  arrange(title_sent) %>%
  select(title,title_sent) %>%
  head(n=5L)
```

Sentiment analysis was a great exercise, but later, when we computed variable importance for our random forest model, we will that its lack of robustness meant that it was not a meaningful predictor for our response variable.

## Section 3: Variable Modification

After retrieving the data, we take multiple steps to wrangle it into a format that makes more sense given the variables we are trying to investigate. The data for "Time" that we retrieve from is in POSIX format, units of seconds elapsed from January 1st, 1970 in UTC time. We thus translate that metric of time into year, date, and hour.

```{r, echo=FALSE}
#Store all of the POSIX dates into a vector
dates <- as.POSIXct(reddit_df$created_utc,origin = "1970-01-01",tz = "UTC")

#Use lubridate to read y,m,d,h from that vector. Create new columns for these
reddit_df <- reddit_df %>%  
  mutate(post_year = year(dates),post_month = month(dates), post_day = day(dates),
         post_hour = hour(dates))
```

After obtaining that information, we create a categorical variable for "time" to categorize the time of day that the post was created into "Day", "Morning", and "Night". This step was necessary because otherwise, the classification algorithm would not classify "23" and "0" as the same portion of the day (time wise) even though they should be. Furthermore, having 24 categories for hour would not be ideal for model building, as that gives too many options for splitting. We perform similar modifications to the "day" column, turning it into the time of month posted, and the "month" column, breaking that into the four seasons. 

```{r, echo=FALSE}
#Create a new column for categorical time of day
#Time ranges: PST 6pm-3am night      3am-10am morning    11am-6pm day
mk_cat_time <- function(h)
{
  ifelse(h>18, "night", ifelse(h>10, "day", ifelse(h>2, "morning", "night")))
}

#Create a new column for categorical time of month
#Time ranges: 1-10 early    11-20 mid    21-31 late
mk_cat_day <- function(d)
{
  ifelse(d>20, "late", ifelse(d>10, "mid", "early"))
}

#Create a new column for categorical time of year
#Time ranges: Dec-Feb: Winter   Mar-May: Spring    Jun-Aug: Summer    Sep-Nov: Fall
mk_cat_month <- function(m)
{
  ifelse(m>11, "Winter", ifelse(m>8, "Fall", 
                                ifelse(m>5, "Summer", ifelse(m>2, "Spring", "Winter"))))
}

reddit_df <- reddit_df %>%
  mutate(cat_post_hour = mk_cat_time(post_hour)) %>%
  mutate(cat_post_day = mk_cat_day(post_day)) %>%
  mutate(cat_post_month = mk_cat_month(post_month)) 
```

Next, we clean up the data in order to remove posts that do not share scientific content (e.g. AMAs, subreddit discussions) by filtering under the subfield column. We also remove all subfields that have less than 100 posts, since these were mostly errorneously named subfields that had only 1 post.

```{r, echo=FALSE}
#Filter by removing the Ask Me Anything (AMA) threads and the subreddit discussion
  #threads since these are not studies. 
reddit_df <- reddit_df %>%
  filter(!str_detect(subfield, " AMA")) %>%
  group_by(subfield) %>%
  filter(n() > 100) %>%
  ungroup()
```

Now, we group the remaining subfields into four major categories so that our model can more easily divide among them. We made these selections by our impressions of the categories that these fields fall into, so this step is certainly subjective. Our funciton for conversion is below to display our categorizations.

```{r}
#Make a function that sorts fields into "Life Science,"
#"Psychology," "Physical Science," "Environment"
mk_cat_subfield <- function(field)
{
  ifelse(field == "Biology" | field =="Health" | field =="Cancer" | 
           field =="Medicine" | field =="Animal Science" | field =="Paleontology" | 
           field =="Epidemiology", "Life Science", 
         
         ifelse(field =="Psychology" |
           field =="Neuroscience" | field =="Social Science" |
            field =="Anthropology", "Psychology",
           
           ifelse(field =="Geology" | field =="Astronomy" | field =="Physics" |
            field =="Chemistry" | field =="Nanoscience" | field =="Engineering" |
            field =="Computer Science", "Physical Science", 
            
            "Environment")))
}
```

```{r, echo=FALSE}
#Apply the function and remove the old column
reddit_df <- reddit_df %>%
  mutate(cat_subfield = mk_cat_subfield(subfield)) %>%
  select(-subfield)

#Turn the new column into a factor for model use
reddit_df$cat_subfield <- factor(reddit_df$cat_subfield)
```

Here we make a pie plot of the distribution of articles into these four categories:

```{r, echo=TRUE}
subfield_freq = table(reddit_df$cat_subfield)
type_freq_perc = paste(round((subfield_freq/length(reddit_df$cat_subfield)),2)*100,
                       "%",sep="")
type_freq_lab = paste(c("Environment","Life science",
                        "Physical science", "Psychology"), type_freq_perc, sep = " ")
pie(subfield_freq,labels=type_freq_lab, init.angle = 51,edges = 10000,
    col = c("paleturquoise3","darkseagreen4","slateblue4","plum2"), main="Distribution of Subfields")
```

Next, we turn the categorical variables to factors, which the caret random forest model expects.
```{r, echo=FALSE}
#Turn all categorical variables into factors
reddit_df$author_flair_binary <- factor(reddit_df$author_flair_binary)
reddit_df$image <- factor(reddit_df$image)
reddit_df$journal_h_index <- factor(reddit_df$journal_h_index)
reddit_df$post_year <- factor(reddit_df$post_year)
reddit_df$cat_post_month <- factor(reddit_df$cat_post_month)
reddit_df$cat_post_hour <- factor(reddit_df$cat_post_hour)
reddit_df$cat_post_day <- factor(reddit_df$cat_post_day)
```

Then we plot select explanatory variables against our response variable to get a sense of any correlations between them.

Author created date vs upvotes. We see a few vertical lines of dots (e.g. the one near 1.15e+09), which we looked into and realized that they are single authors that are prolific on Reddit! Since these outliers could bias our model's prediction vis-a-vis author_created_date, we later remove that column from the dataframe.

```{r, echo=TRUE}
acd_plot <- ggplot(data=reddit_df, aes(x=author_created_date, y=upvotes))+
  geom_point(size=0.5) +
  xlab("Author Creation Date")+
  ylab("Upvotes")+
  ggtitle("Upvotes v.s. Author Creation Date")
ggsave(acd_plot, filename='./acd_plot.png', width=8, height=5)

acd_plot
```

Number of comments vs upvotes. This is an example of a variable that shows strong positive correlation with our response variable. Indeed, we could have used number of comments instead of upvotes as our response variable and achieved a very similar model. To create a fair and meaningful prediction model, we thus remove num_comments and other highly correlated variables (link_karma,comment_karma, and gilded).

```{r, echo=TRUE}
nc_plot <- ggplot(data=reddit_df, aes(x=num_comments, y=upvotes))+
  geom_point(size=0.5) +
  xlab("Number of Comments")+
  ylab("Upvotes")+
  ggtitle("Upvotes v.s. Number of Comments")
ggsave(nc_plot, filename='./nc_plot.png', width=8, height=5)

nc_plot
```

The other columns removed below are due to conversions to categorical variables (i.e. post_hour is now irrelevant since we have cat_post_hour, breaking that variable into three times of day).

```{r, echo=TRUE}
#To prepare for model building, we remove unnecessary columns
reddit_df <- reddit_df %>%
  select(-X1,-author,-id,-created_utc,-domain,-url,-author_flair,-post_day,
         -post_hour,-post_month,-num_comments,-gilded,-link_karma,
         -comment_karma,-title,-X1_1,-author_created_date)
```

Now we remove any rows that have missing values (NAs). We also print the proportion of rows removed below to ensure that we are not removing a large fraction of the data. It is comforting to see that less than 0.03% of our observations contained any NAs, so it is unlikely that we are introducing significant bias by removing these. 

```{r, echo=FALSE}
bef_rm <- nrow(reddit_df)

#Remove missing values
reddit_df <- reddit_df[complete.cases(reddit_df),]
aft_rm <- nrow(reddit_df)

paste("Proportion of rows removed: ",(bef_rm-aft_rm)/bef_rm)
```

Here we plot a histogram of the number of upvotes, showing that the vast majority of our data has "low" (<1000) upvotes. The huge range (from 0 to 100000) and high level of skewedness of upvotes motivate us to separate our response into a binary variable. Before we do so, we examine two more plots:

```{r, echo=TRUE}
hist_upvotes <- qplot(log10(reddit_df$upvotes+1),
      geom="histogram",
      binwidth = 0.05,
      main = 'Histogram of Upvotes (Response Variable)',
      xlab = "log(Number of Upvotes)",
      ylab = "Counts",
      fill=I("blue"),
      col=I("red"), 
      alpha=I(.2),
      xlim=c(0,5)) +
  geom_vline(xintercept=3,show_guide=TRUE,linetype="dashed")

hist_upvotes
#ggsave(hist_upvotes, filename='./hist_upvotes.png', width=8, height=5)
```

Here, we plot the mean title word length vs. number of upvotes. There appears to be a sweet spot around 5. It appears that lower mean title word lengths give higher upvotes. There are, however, more posts near the low range too, so we cannot make that conclusion just yet.

```{r, echo=TRUE}
mtl_plot <- ggplot(data=reddit_df, aes(x=mean_title_length, y=upvotes))+
  geom_point(size=0.5) +
  xlab("Average Word Length")+
  ylab("Upvotes")+
  ggtitle("Upvotes v.s. Average Word Length (in Title)")

mtl_plot
#ggsave(mtl_plot, filename='./mtl_plot.png', width=8, height=5)
```

Here, we plot the total title length vs. number of upvotes. No strong correlation could be seen easily by eye. 

```{r, echo=TRUE}
tl_plot <- ggplot(data=reddit_df, aes(x=title_len, y=upvotes))+
  geom_point(size=0.5) +
  xlab("Total Title Length")+
  ylab("Upvotes")+
  ggtitle("Upvotes v.s. Total Title Length")
ggsave(tl_plot, filename='./tl_plot.png', width=8, height=5)

tl_plot
```

Now we turn the continuous upvote response variable into a binary response. We choose 1000 upvotes as our cut-off, because that is the approximate threshold where a post will end up on the front page of the Science subreddit, from which it could reach the front page of the entire website.

```{r, echo=TRUE}
#Funciton to create a new column for categorical upvotes
#Ranges: 1000+ high      0+ low
mk_cat_upvotes <- function(upv)
{
  ifelse(upv>1000, "High", "Low")
}

#Apply the function and remove the old column
reddit_df <- reddit_df %>%
  mutate(cat_upvotes = mk_cat_upvotes(upvotes)) %>%
  select(-upvotes)

#Turn the column into a factor
reddit_df$cat_upvotes <- factor(reddit_df$cat_upvotes)
```

## Section 4: Model Building and Assessment

After wrangling the data, we are ready to build our Random Forest model. We partition the data into test (20%) and training (80%) sets, setting aside the test data for later use to appropriately assess our model's accuracy. We did not apply this test data until after finalizing and tuning our model, so it will be an independent assessment of accuracy. 

```{r, echo=TRUE}
#Set the seed for reproducibility
set.seed(47)

#Split the data into test and training, with 80% going to training
inTrain <- createDataPartition(y = reddit_df$cat_upvotes, p=0.80, list=FALSE)
reddit.train <- reddit_df[inTrain,]
reddit.test <- reddit_df[-c(inTrain),]
```

By looking at the number of observations in each category below, we see that predicting everything as "low" gives a fairly high accuracy. In other words, for the model, there is little consequence in masclassifying the posts with "high" upvotes. To avoid this problem in building our model, we sample from our training set to have an equal number of observations with "high" and "low" upvotes. In doing so, we force the model to find signal to separate the two populations rather than predicting everything as low. 

```{r, echo=FALSE}
#Print the number of observations in each category
paste("Number of high upvotes: ",sum(reddit.train['cat_upvotes'] == "High"))
paste("Number of low upvotes: ",sum(reddit.train['cat_upvotes'] == "Low"))
```

```{r, echo=TRUE}
### Sample data such that there is a equal number of observations for each category.
set.seed(4747)

#Count the number of high upvotes
num_high_upvote <- sum(reddit.train['cat_upvotes'] == "High")

reddit.train.high <- subset(reddit.train, reddit.train['cat_upvotes'] == "High")
reddit.train.low <- subset(reddit.train, reddit.train['cat_upvotes'] == "Low")

#Select an equal proportion of high and low upvotes
inTrain.low <- createDataPartition(y = reddit.train.low$cat_upvotes,
                                   p=num_high_upvote/dim(reddit.train.low)[1],
                                   list=FALSE)
reddit.train.low <- reddit.train.low[inTrain.low,]

### Combine the 2 dataframes into the final training data
reddit.train.final <- rbind(reddit.train.low, reddit.train.high)
```

Below we see the final number of training observations to be used by the model and verify that there are an equal proportion of "high" and "low" upvotes.

```{r, echo=FALSE}
#Check that the numbers are equal.
reddit.train.final %>% group_by(cat_upvotes) %>% summarize(n())
```

Now we grow the random forest of 1000 trees on this training set, using the out of bag (OOB) error rate to tune the `mtry` parameter (number of variables at each split). We let mtry be an odd number from 3-11, where these values were chosen to hopefull find some signal (mtry=1 may not be particularly useful since it never directly compares two variables at a given split) and doesn't consider too much of the data at every split (we have 14 explanatory variables).

```{r, echo=TRUE}
#Build the random forest model. This chunk takes ~3 minutes to run
set.seed(4747)

rf.reddit <- train(cat_upvotes ~., data=reddit.train.final, method="rf",
                      trControl = trainControl(method="oob"),
                      ntree=1000, tuneGrid = data.frame(mtry=c(3,5,7,9,11)),
                      importance = TRUE,na.action = na.exclude)
```

The value of mtry does not appear to have a large impact on our accuracy as seen below, but mtry=3 gives the greatest oob prediction rate, so we build our final model with that value. 

```{r, echo=FALSE}
rf.reddit
```

A more in-depth look at our final model is below, where we see that it predicted more observations as "high" than "low," leading to a lower error rate for "high" upvoted posts than "low" upvoted. 

```{r, echo=FALSE}
rf.reddit$finalModel
```

Using this best model, we predict our test data to assess our model's accuracy. 

```{r, echo=FALSE}
confusionMatrix(data=predict(rf.reddit, newdata = reddit.test), reference = reddit.test$cat_upvotes)
```

The confidence interval for the accuracy of the model is between 0.6094 and 0.6388, which is greater than 50%: this suggests that there \textit{is} some information in this model, i.e., that there is a basis on which to distinguish popular posts from less popular posts. With that in mind, we print the variable importance from the model, which indicates which explanatory variables most directly impact the response. These values are computed by examining the decrease in node purity when permuting the values of a particular variable across the dataset. If permuting the values of a variable causes node to be significantly more impure, then that variable is likely very important in distinguisihing the response.  

```{r, echo=FALSE}
varImp(rf.reddit,scale=FALSE)
```

Of the variables that we use in our model, the output above suggests that the four most important variables contributing to the popularity of a post are: 

1. Length of the title of the post (longer better)

2. Whether there is a thumbnail accompanying the post (thumbnail better)

3. The average length of words in the title (shorter better)

4. The $h$ index of content linked to in the post (lower better)

Beyond this, there is a larger dropoff before the next variable, so it is safe to conclude that these are the most important four variables accroding to our model. 

The first one might be because long post titles that summarize the content of the article allow readers to get their information without having to click through the article, causing them to upvote. Thumbnails give an article more legitimacy and catch the reader's attention on the webpage, so the second one also makes sense. The third and the fourth could be interpreted together, suggesting that readers are looking for a simpler presentation of the content. Having a rigorous, academic seems to negatively impact upvotes, so linking to a popular science summary of such an article might garner more attention. Also, the shorter average word length suggests that readers dislike long scientific jargon and would rather have the article presented in more digestable terms. 

Beyond these top four, we see that the subfield that the article is about is mostly irrelevant. This seems to suggest that the presentation of the article - rather than the content - attracts readers. 

## Section 5: Conclusions

In this project, we attempt to look for variables that may affect whether a science post on Reddit gathers a large number of upvotes. Per the previous section, we find that the most important variables are 1. post title length, 2. whether the post has a thumbnail, 3. the average word length in the title, and 4. the $h$-index of the post. In composite, these results suggest that users prefer a kind of "fast-food" model of scientific media consumption, in which information is conveyed and digested in small, colloquial chunks. 

It is somewhat surprising that subfield, our one proxy for content, is not a strong predictor in our model. However, it seems naive to claim that content does not play a role in the popularity of a post. The limiting part of our study was that we couldn't extract more information about the posts, and what we did try was ineffective. Part of the reason may be that the title of the posts were too short, and thus our "sentiment density" calculation suffered from small-number statistics. Future work could entail performing the same kind of seniment analysis on the entire article, although that would be computationally expensive. In a different vein, we could also look into using an algorithm, or method, that takes into account how words in the title interact with each other to create meaning (some form of topic analysis? Although we would then need to be cautious of creating topic categories that are redundant with the "subfield" category).

We also want to note that our data certainly isn't representative of the population, as Reddit users are primarily college-aged students and male. Moreover, we caution against claiming that our data is representative of Reddit users more generally, as only 4% of Reddit users are subscribed to the "Science" subreddit. Although the data isn't representative, however, this result also makes sense given our individual experiences with scientific communication and outreach. That's reassuring: our intuitions seem to have been confirmed by analysis on a large body of data. 

Unfortunately, due to the limitations of our study, we could not draw more specific conclusions about what makes a post popular on Reddit, and therefore cannot make more specific suggestions as to what works in scientific communication. However, completing this project has been an educational experience in data analysis. We have done research before, but worked only with astrophysical data, so venturing into the world of social media analysis was new for all of us (certainly, none of us had to think about sentiment analysis when trying to understand photon counts). As budding scientists, it was important to us that we think about how to connect the knowledge that we produce in our ivory towers to those who live outside of it: doing this project has allowed us to consider this issue from a statistical perspective.